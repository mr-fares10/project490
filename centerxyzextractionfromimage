import torch
import cv2
import numpy as np
from PIL import Image
import os
import torchvision
from torchvision import transforms
import json
from datetime import datetime

# Check if CUDA is available and set device accordingly
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load models with updated model names and error handling
try:
    # For newer torchvision versions, model names might have changed
    try:
        # Try the newest naming convention first
        seg_model = torch.hub.load('pytorch/vision', 'maskrcnn_resnet50_fpn', weights='DEFAULT')
    except Exception as e:
        print(f"Trying alternate model name: {e}")
        try:
            # Try the alternate naming convention
            seg_model = torch.hub.load('pytorch/vision:v0.10.0', 'maskrcnn_resnet50_fpn', pretrained=True)
        except Exception as e:
            print(f"Trying fallback method: {e}")
            # Fallback to direct torchvision models import
            seg_model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

    # Load MiDaS model
    try:
        depth_model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')
    except Exception as e:
        print(f"Error loading MiDaS: {e}")
        print("Attempting to load alternate MiDaS model...")
        # Try alternate repository
        depth_model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')

    # Move models to the appropriate device
    seg_model.to(device)
    depth_model.to(device)
    # Set models to evaluation mode
    seg_model.eval()
    depth_model.eval()
    print("Models loaded successfully!")
except Exception as e:
    print(f"Error loading models: {e}")
    raise

# Dictionary of common aviation camera specifications
AVIATION_CAMERAS = {
    # Professional cameras often used for aviation documentation
    "Sony Alpha 7R IV": {
        "sensor_width": 35.7,  # mm
        "sensor_height": 23.8,  # mm
        "resolution": (9504, 6336),  # pixels
        "pixel_size": 0.00375,  # mm
        "typical_focal_length": 85  # mm (common lens for aviation)
    },
    "Canon EOS 5D Mark IV": {
        "sensor_width": 36.0,  # mm
        "sensor_height": 24.0,  # mm
        "resolution": (6720, 4480),  # pixels
        "pixel_size": 0.00535,  # mm
        "typical_focal_length": 100  # mm
    },
    "Nikon D850": {
        "sensor_width": 35.9,  # mm
        "sensor_height": 23.9,  # mm
        "resolution": (8256, 5504),  # pixels
        "pixel_size": 0.00435,  # mm
        "typical_focal_length": 70  # mm
    },
    # Military-grade reconnaissance cameras
    "MS-110 Reconnaissance Pod": {
        "sensor_width": 49.2,  # mm (approximation)
        "sensor_height": 36.8,  # mm
        "resolution": (9000, 6000),  # pixels (approximation)
        "pixel_size": 0.00546,  # mm
        "typical_focal_length": 152  # mm
    },
    "SYERS-2 Sensor": {
        "sensor_width": 55.0,  # mm (approximation)
        "sensor_height": 41.0,  # mm
        "resolution": (10000, 8000),  # pixels (approximation)
        "pixel_size": 0.0055,  # mm
        "typical_focal_length": 210  # mm
    }
}

# Dictionary of fighter jet reference dimensions (in meters)
FIGHTER_JET_DIMENSIONS = {
    "F-16 Fighting Falcon": {
        "length": 15.06,
        "wingspan": 9.96,
        "height": 5.09,
        "empty_weight_kg": 8570
    },
    "F/A-18 Hornet": {
        "length": 17.07,
        "wingspan": 13.62,
        "height": 4.66,
        "empty_weight_kg": 10455
    },
    "F-22 Raptor": {
        "length": 18.92,
        "wingspan": 13.56,
        "height": 5.08,
        "empty_weight_kg": 19700
    },
    "F-35 Lightning II": {
        "length": 15.67,
        "wingspan": 10.7,
        "height": 4.38,
        "empty_weight_kg": 13290
    },
    "Su-27 Flanker": {
        "length": 21.94,
        "wingspan": 14.7,
        "height": 5.93,
        "empty_weight_kg": 16380
    },
    "MiG-29 Fulcrum": {
        "length": 17.32,
        "wingspan": 11.36,
        "height": 4.73,
        "empty_weight_kg": 11000
    },
    "Eurofighter Typhoon": {
        "length": 15.96,
        "wingspan": 10.95,
        "height": 5.28,
        "empty_weight_kg": 11000
    },
    "Rafale": {
        "length": 15.27,
        "wingspan": 10.8,
        "height": 5.34,
        "empty_weight_kg": 10000
    },
    "J-20": {
        "length": 20.3,
        "wingspan": 12.88,
        "height": 4.45,
        "empty_weight_kg": 17000
    }
}

def calculate_camera_parameters(image_shape, camera_model="Sony Alpha 7R IV", focal_length_mm=None):
    """
    Calculate camera intrinsic parameters based on camera specifications

    Parameters:
    - image_shape: tuple of (height, width) of the image
    - camera_model: string name of camera model from AVIATION_CAMERAS
    - focal_length_mm: actual focal length in mm, if different from typical

    Returns:
    - dict with fx, fy (focal lengths in pixels), cx, cy (principal point)
    """
    if camera_model not in AVIATION_CAMERAS:
        print(f"Warning: Camera model {camera_model} not found in database. Using default values.")
        camera_model = "Sony Alpha 7R IV"  # Default to a common camera

    camera_specs = AVIATION_CAMERAS[camera_model]

    # Use provided focal length or typical value
    actual_focal_length = focal_length_mm if focal_length_mm else camera_specs["typical_focal_length"]

    # Calculate scaling factor if image resolution differs from camera's native resolution
    width_scale = image_shape[1] / camera_specs["resolution"][0]
    height_scale = image_shape[0] / camera_specs["resolution"][1]

    # Convert focal length from mm to pixels
    # Formula: focal_px = focal_mm / pixel_size_mm
    focal_length_x_pixels = actual_focal_length / camera_specs["pixel_size"] * width_scale
    focal_length_y_pixels = actual_focal_length / camera_specs["pixel_size"] * height_scale

    # Principal point (usually center of image)
    cx = image_shape[1] / 2
    cy = image_shape[0] / 2

    return {
        "fx": focal_length_x_pixels,
        "fy": focal_length_y_pixels,
        "cx": cx,
        "cy": cy,
        "camera_model": camera_model,
        "focal_length_mm": actual_focal_length
    }

def process_fighter_jet_image(image_path, camera_model="Sony Alpha 7R IV", focal_length_mm=None,
                             known_distance=None, known_jet_type=None, altitude=None, camera_angle=None):
    """
    Process a fighter jet image to extract dimensions and 3D coordinates in meters.

    Parameters:
    - image_path: Path to the fighter jet image
    - camera_model: Camera model used for the image
    - focal_length_mm: Actual focal length in mm
    - known_distance: Known distance to the aircraft in meters (if available)
    - known_jet_type: Type of fighter jet if known (for dimension reference)
    - altitude: Altitude of the camera in meters (if available)
    - camera_angle: Angle of the camera relative to horizontal in degrees

    Returns:
    - Dictionary containing detection results and dimensions
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    # Load image
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"Failed to load image: {image_path}")

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    original_height, original_width = image.shape[:2]

    # Get camera parameters
    camera_params = calculate_camera_parameters(
        (original_height, original_width),
        camera_model,
        focal_length_mm
    )

    fx, fy = camera_params["fx"], camera_params["fy"]
    cx, cy = camera_params["cx"], camera_params["cy"]

    print(f"Image dimensions: {original_width}x{original_height}")
    print(f"Camera: {camera_model}, Focal Length: {camera_params['focal_length_mm']}mm")
    print(f"Camera parameters: fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}")

    # Step 1: Object Segmentation with proper preprocessing
    seg_tensor = transforms.ToTensor()(image_rgb).to(device)

    # Run segmentation model
    with torch.no_grad():
        prediction = seg_model([seg_tensor])

    # Check if any objects detected
    scores = prediction[0]['scores']
    if len(scores) == 0:
        print("No objects detected in the image")
        return None

    # Filter predictions with confidence > 0.5
    high_conf_indices = torch.where(scores > 0.5)[0]
    if len(high_conf_indices) == 0:
        print("No objects detected with confidence > 0.5")
        return None

    # Get COCO class names
    COCO_INSTANCE_CATEGORY_NAMES = [
        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
    ]

    # Look for airplane class (index 4)
    airplane_indices = [i for i in high_conf_indices if prediction[0]['labels'][i].item() == 4]

    if airplane_indices:
        # Prioritize airplane detections
        best_idx = airplane_indices[0].item()
        print("Fighter jet (airplane class) detected!")
    else:
        # If no airplane detected, use best detection
        best_idx = high_conf_indices[0].item()
        print("No fighter jet detected, using best available object")

    mask = prediction[0]['masks'][best_idx, 0].cpu().numpy()
    score = scores[best_idx].item()
    label = prediction[0]['labels'][best_idx].item()
    class_name = COCO_INSTANCE_CATEGORY_NAMES[label]

    print(f"Detected {class_name} with confidence: {score:.2f}")

    # Check if we actually found a fighter jet/airplane
    if class_name != 'airplane' and known_jet_type:
        print(f"Warning: Expected fighter jet but detected {class_name}")
        print(f"Proceeding anyway with {known_jet_type} reference dimensions")

    # Threshold mask
    binary_mask = mask > 0.5
    y_indices, x_indices = np.where(binary_mask)
    if len(x_indices) == 0 or len(y_indices) == 0:
        print("Segmentation mask is empty after thresholding")
        return None

    # Compute 2D centroid
    u_centroid = np.mean(x_indices)
    v_centroid = np.mean(y_indices)

    # Compute object bounding box
    min_x, max_x = np.min(x_indices), np.max(x_indices)
    min_y, max_y = np.min(y_indices), np.max(y_indices)

    # Extract key dimensions in pixels
    width_px = max_x - min_x
    height_px = max_y - min_y
    diagonal_px = np.sqrt(width_px**2 + height_px**2)

    # Compute the oriented bounding box to get more accurate aircraft dimensions
    # This helps with aircraft at an angle
    from cv2 import minAreaRect, boxPoints
    points = np.column_stack((x_indices, y_indices))
    rect = minAreaRect(points)
    box = boxPoints(rect)
    box = np.int0(box)

    # Get rotated rectangle dimensions
    rect_width, rect_height = rect[1]
    rot_angle = rect[2]

    # Ensure width is always the longer dimension for aircraft (length)
    if rect_width < rect_height:
        rect_width, rect_height = rect_height, rect_width
        rot_angle = 90 - rot_angle

    print(f"2D Centroid (u, v): ({u_centroid:.2f}, {v_centroid:.2f})")
    print(f"Aircraft dimensions in pixels: {rect_width:.1f} × {rect_height:.1f}")
    print(f"Aircraft orientation: {rot_angle:.1f} degrees")

    # Visualize segmentation
    visualization = image_rgb.copy()
    # Draw mask overlay
    mask_overlay = np.zeros_like(visualization)
    mask_overlay[binary_mask, 0] = 255  # Red channel
    mask_overlay = cv2.addWeighted(visualization, 1, mask_overlay, 0.5, 0)
    # Draw centroid
    cv2.circle(mask_overlay, (int(u_centroid), int(v_centroid)), 5, (0, 255, 0), -1)
    # Draw oriented bounding box
    cv2.drawContours(mask_overlay, [box], 0, (0, 255, 255), 2)

    # Add measurement annotations
    cv2.putText(mask_overlay, f"Length: {rect_width:.1f}px",
                (int(min_x), int(min_y - 10)),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    cv2.putText(mask_overlay, f"Width: {rect_height:.1f}px",
                (int(min_x), int(min_y - 40)),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

    # Step 2: Depth Estimation with MiDaS preprocessing
    depth_transform = transforms.Compose([
        transforms.Resize((384, 384)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Convert and preprocess image
    pil_image = Image.fromarray(image_rgb)
    depth_input = depth_transform(pil_image).unsqueeze(0).to(device)

    # Run depth model
    with torch.no_grad():
        depth_pred = depth_model(depth_input)

    depth_map = depth_pred.squeeze().cpu().numpy()

    # Normalize depth map for visualization
    depth_vis = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    depth_vis = (depth_vis * 255).astype(np.uint8)
    depth_vis = cv2.resize(depth_vis, (original_width, original_height), interpolation=cv2.INTER_CUBIC)

    # Resize depth map to original image size for calculation
    depth_map_resized = cv2.resize(depth_map, (original_width, original_height), interpolation=cv2.INTER_CUBIC)

    # Extract depth for aircraft
    aircraft_relative_depth = depth_map_resized[y_indices, x_indices].mean()

    # Convert MiDaS relative depth to meters using calibration methods

    # Method 1: If we know the actual distance
    if known_distance is not None:
        depth_scale_factor = known_distance / aircraft_relative_depth
        aircraft_depth_meters = known_distance
        print(f"Using known distance calibration: {known_distance:.2f}m")

    # Method 2: If we know the aircraft type, use its dimensions for scale
    elif known_jet_type is not None and known_jet_type in FIGHTER_JET_DIMENSIONS:
        jet_specs = FIGHTER_JET_DIMENSIONS[known_jet_type]

        # Use length (assuming forward-facing camera)
        reference_length = jet_specs["length"]

        # Calculate actual distance using pinhole camera model
        # real_length / image_length = distance / focal_length
        aircraft_depth_meters = (reference_length * fx) / rect_width
        depth_scale_factor = aircraft_depth_meters / aircraft_relative_depth

        print(f"Using {known_jet_type} reference dimensions for calibration")
        print(f"Reference length: {reference_length:.2f}m")

    # Method 3: If we have altitude information
    elif altitude is not None and camera_angle is not None:
        # Convert angle to radians
        angle_rad = np.radians(camera_angle)

        # Calculate distance using trigonometry
        # distance = altitude / sin(angle)
        aircraft_depth_meters = altitude / np.sin(angle_rad)
        depth_scale_factor = aircraft_depth_meters / aircraft_relative_depth

        print(f"Using altitude ({altitude:.2f}m) and camera angle ({camera_angle:.1f}°) for calibration")

    # Method 4: Fallback to approximate scale based on typical aircraft
    else:
        # Use average fighter jet length (~17m) as reference
        avg_fighter_length = 17.0

        # Estimate distance
        aircraft_depth_meters = (avg_fighter_length * fx) / rect_width
        depth_scale_factor = aircraft_depth_meters / aircraft_relative_depth

        print("Using average fighter jet dimensions for approximate calibration")
        print("WARNING: For accurate measurements, provide known_distance or known_jet_type")

    print(f"Estimated distance to aircraft: {aircraft_depth_meters:.2f} meters")

    # Step 3: Calculate aircraft dimensions in meters
    # Using the pinhole camera model: size_meters = (size_pixels * distance) / focal_length

    # Calculate dimensions using oriented bounding box
    length_meters = (rect_width * aircraft_depth_meters) / fx
    wingspan_meters = (rect_height * aircraft_depth_meters) / fx

    # Step 4: Convert 2D centroid to 3D coordinates in meters
    x_meters = (u_centroid - cx) * aircraft_depth_meters / fx
    y_meters = (v_centroid - cy) * aircraft_depth_meters / fy
    z_meters = aircraft_depth_meters

    # Determine aircraft type based on dimensions if not provided
    estimated_type = None
    if not known_jet_type:
        best_match = None
        smallest_diff = float('inf')

        for jet_type, specs in FIGHTER_JET_DIMENSIONS.items():
            # Calculate difference between estimated and reference dimensions
            length_diff = abs(length_meters - specs["length"]) / specs["length"]
            wingspan_diff = abs(wingspan_meters - specs["wingspan"]) / specs["wingspan"]

            # Combined difference score (weighted more on length as it's usually more accurate)
            total_diff = length_diff * 0.6 + wingspan_diff * 0.4

            if total_diff < smallest_diff and total_diff < 0.25:  # Within 25% of reference
                smallest_diff = total_diff
                best_match = jet_type

        if best_match:
            estimated_type = best_match
            print(f"Aircraft dimensions match {estimated_type} (confidence: {(1-smallest_diff)*100:.1f}%)")

    # Create detailed results
    result = {
        "detection": {
            "class": class_name,
            "confidence": score,
            "2d_centroid_px": (u_centroid, v_centroid),
            "3d_position_meters": (x_meters, y_meters, z_meters),
            "distance_meters": aircraft_depth_meters,
            "orientation_degrees": rot_angle
        },
        "dimensions": {
            "length_px": rect_width,
            "wingspan_px": rect_height,
            "length_meters": length_meters,
            "wingspan_meters": wingspan_meters,
            "aspect_ratio": rect_width / rect_height if rect_height > 0 else 0
        },
        "aircraft": {
            "type": known_jet_type if known_jet_type else estimated_type,
            "estimated_type": estimated_type if not known_jet_type else None,
        },
        "camera": {
            "model": camera_model,
            "focal_length_mm": camera_params["focal_length_mm"],
            "fx_px": fx,
            "fy_px": fy
        },
        "calibration": {
            "depth_scale_factor": depth_scale_factor,
            "calibration_method": "known_distance" if known_distance else
                                "known_aircraft_type" if known_jet_type else
                                "altitude_angle" if altitude and camera_angle else
                                "approximate"
        },
        "visualizations": {
            "segmentation": mask_overlay,
            "depth": depth_vis
        },
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    # If we have a reference aircraft, compare our measurements
    if known_jet_type and known_jet_type in FIGHTER_JET_DIMENSIONS:
        jet_specs = FIGHTER_JET_DIMENSIONS[known_jet_type]
        result["reference"] = {
            "length_meters": jet_specs["length"],
            "wingspan_meters": jet_specs["wingspan"],
            "height_meters": jet_specs["height"],
            "length_error_pct": abs(length_meters - jet_specs["length"]) / jet_specs["length"] * 100,
            "wingspan_error_pct": abs(wingspan_meters - jet_specs["wingspan"]) / jet_specs["wingspan"] * 100
        }

    return result

# Helper function to save results
def save_results(result, output_dir="./output"):
    """Save detection results and visualizations to disk"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Save visualizations
    cv2.imwrite(f"{output_dir}/segmentation_{timestamp}.jpg",
                cv2.cvtColor(result["visualizations"]["segmentation"], cv2.COLOR_RGB2BGR))
    cv2.imwrite(f"{output_dir}/depth_map_{timestamp}.jpg",
                result["visualizations"]["depth"])

    # Save metadata and measurements
    result_copy = result.copy()
    # Remove visualizations from JSON (too large)
    del result_copy["visualizations"]

    with open(f"{output_dir}/measurements_{timestamp}.json", 'w') as f:
        json.dump(result_copy, f, indent=2)

    print(f"Results saved to {output_dir}/")
    return f"{output_dir}/measurements_{timestamp}.json"

# Main execution
if __name__ == "__main__":
    try:
        # Path to your fighter jet image
        image_path = "/content/GEM_Fighter-Jets_01.jpg"

        # Example usage with different cameras and calibration methods

        # Option 1: Using a known fighter jet type for calibration
        result = process_fighter_jet_image(
            image_path,
            camera_model="Sony Alpha 7R IV",
            focal_length_mm=200,  # Common telephoto lens for aviation photography
            known_jet_type="F-16 Fighting Falcon"
        )

        # Option 2: Using known distance for calibration
        # result = process_fighter_jet_image(
        #     image_path,
        #     camera_model="Nikon D850",
        #     focal_length_mm=400,
        #     known_distance=350  # meters
        # )

        # Option 3: Using altitude and camera angle
        # result = process_fighter_jet_image(
        #     image_path,
        #     camera_model="MS-110 Reconnaissance Pod",
        #     focal_length_mm=300,
        #     altitude=3000,  # meters
        #     camera_angle=45  # degrees from horizontal
        # )

        if result:
            print("\nResults:")
            print(f"Aircraft: {result['aircraft']['type'] or 'Unknown type'}")
            print(f"Distance: {result['detection']['distance_meters']:.2f} meters")
            print(f"Dimensions: {result['dimensions']['length_meters']:.2f}m × {result['dimensions']['wingspan_meters']:.2f}m")
            print(f"3D Position (meters): X={result['detection']['3d_position_meters'][0]:.2f}, Y={result['detection']['3d_position_meters'][1]:.2f}, Z={result['detection']['3d_position_meters'][2]:.2f}")

            # Save results
            save_results(result)

    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
